{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.6",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "<img style=\"text-aling:left;\" src=\"https://github.com/Microsoft/sqlworkshops/blob/master/graphics/solutions-microsoft-logo-small.png?raw=true\" alt=\"Microsoft\">\r\n",
                "<br>\r\n",
                "\r\n",
                "# Machine Learning and the Team Data Science Process \r\n",
                "## http://aka.ms/tdsp\r\n",
                "\r\n",
                "This Jupyter Notebook walks through the various steps of the Team Data Science Process (TDSP) using Machine Learning in Python. We'll walk through the Phases of the Team Data Science Process to predict a customer churn number - something quite common as a use-case. \r\n",
                "\r\n",
                "## Phase One: Business Understanding\r\n",
                "The Orange Telecom company in France is one of the largest operators of mobile and internet services in Europe and Africa and a global leader in corporate telecommunication services. They have 256 million customers worldwide. They have significant coverage in France, Spain, Belgium, Poland, Romania, Slovakia Moldova, and a large presence Africa and the Middle East. Customer Churn is always an issue in any company. Orange would like to predict the propensity of customers to switch provider (churn), buy new products or services (appetency), or buy upgrades or add-ons proposed to them to make the sale more profitable (up-selling). For this effort, they think churn is the first thing they would like to focus on.\r\n",
                "\r\n",
                "In this Jupyter Notebook, you'll create, train and store a Machine Learning model using SciKit-Learn, so that it can be deployed to multiple hosts. \r\n",
                "\r\n",
                "Let's start by bringing in the libraries we'll use for machine learning in Python:"
            ],
            "metadata": {
                "azdata_cell_guid": "4817333c-ddba-436b-87f0-18bf4c700899"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import pickle\r\n",
                "import pandas as pd\r\n",
                "import numpy as np\r\n",
                "import csv\r\n",
                "from sklearn.naive_bayes import GaussianNB\r\n",
                "from sklearn.tree import DecisionTreeClassifier\r\n",
                "from sklearn.metrics import accuracy_score\r\n",
                "from sklearn.model_selection import train_test_split\r\n",
                "from sklearn.preprocessing import LabelEncoder\r\n",
                "print(\"Libraries Loaded.\")"
            ],
            "metadata": {
                "azdata_cell_guid": "63229a7a-5463-4e32-b937-1f4523bf8b9d"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Libraries Loaded.\n"
                }
            ],
            "execution_count": 11
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Phase Two: Data Acquisition and Understanding\r\n",
                "\r\n",
                "The Data Aquisition and Understanding phase of the TDSP you ingest or access data from various locations to answer the questions the organization has asked. In most cases, this data will be in multiple locations. Once the data is ingested into the system, you’ll need to examine it to see what it holds. All data needs cleaning, so after the inspection phase, you’ll replace missing values, add and change columns. You’ll cover more extensive Data Wrangling tasks in other labs.\r\n",
                "\r\n",
                "In this section, we’ll use a single file-based dataset to train our model which the company provided. We'll then explore that data a bit, which is often done with graphical outputs as well:"
            ],
            "metadata": {
                "azdata_cell_guid": "525205cf-67f6-471b-8442-f5949eff3bbd"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Read customer data from a single file\r\n",
                "df = pd.read_csv('https://cs7a9736a9346a1x44c6xb00.blob.core.windows.net/backups/CATelcoCustomerChurnTrainingSample.csv', header=0)\r\n",
                "\r\n",
                "# Ensure that you have 29 columns and 20,468 rows loaded\r\n",
                "print('Data Loaded. There should be 20468 obervations of 29 variables:')\r\n",
                "print(df.shape, '\\n')\r\n",
                "\r\n",
                "# Show the size and shape of data:\r\n",
                "print('The size of the data is: %d rows and  %d columns' % df.shape, '\\n')\r\n",
                "\r\n",
                "# Show the first and last 10 rows\r\n",
                "# print('First ten rows of the data: ')\r\n",
                "# print(df.head(10), '\\n')\r\n",
                "# print('Last ten rows of the data: ')\r\n",
                "# print(df.tail(10), '\\n')\r\n",
                "\r\n",
                "# Show the dataframe structure:\r\n",
                "print('Dataframe Structure: ', '\\n')\r\n",
                "print(df.info(), '\\n')\r\n",
                "\r\n",
                "# Check for missing values:\r\n",
                "# print('Missing values: ', '\\n')\r\n",
                "# print(df.apply(lambda x: sum(x.isnull()),axis=0), '\\n') \r\n",
                "\r\n",
                "# perform a simple statistical display:    \r\n",
                "# print('Dataframe Statistics: ', '\\n')\r\n",
                "# print(df.describe(), '\\n')"
            ],
            "metadata": {
                "azdata_cell_guid": "362faeea-0a22-423d-a322-b46443d2f94d"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Data Loaded. There should be 20468 obervations of 29 variables:\n(20468, 29) \n\nThe size of the data is: 20468 rows and  29 columns \n\nDataframe Structure:  \n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 20468 entries, 0 to 20467\nData columns (total 29 columns):\nage                                     20468 non-null int64\nannualincome                            20468 non-null int64\ncalldroprate                            20468 non-null float64\ncallfailurerate                         20468 non-null float64\ncallingnum                              20468 non-null int64\ncustomerid                              20468 non-null int64\ncustomersuspended                       20468 non-null object\neducation                               20468 non-null object\ngender                                  20468 non-null object\nhomeowner                               20468 non-null object\nmaritalstatus                           20468 non-null object\nmonthlybilledamount                     20468 non-null int64\nnoadditionallines                       20468 non-null object\nnumberofcomplaints                      20468 non-null int64\nnumberofmonthunpaid                     20468 non-null int64\nnumdayscontractequipmentplanexpiring    20468 non-null int64\noccupation                              20468 non-null object\npenaltytoswitch                         20468 non-null int64\nstate                                   20468 non-null object\ntotalminsusedinlastmonth                20468 non-null int64\nunpaidbalance                           20468 non-null int64\nusesinternetservice                     20468 non-null object\nusesvoiceservice                        20468 non-null object\npercentagecalloutsidenetwork            20468 non-null float64\ntotalcallduration                       20468 non-null int64\navgcallduration                         20468 non-null int64\nchurn                                   20468 non-null int64\nyear                                    20468 non-null int64\nmonth                                   20468 non-null int64\ndtypes: float64(3), int64(16), object(10)\nmemory usage: 4.5+ MB\nNone \n\n"
                }
            ],
            "execution_count": 15
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Phase Three: Modeling\r\n",
                "\r\n",
                "In this phase, we'll create the experiment runs, perform feature engineering, and run experiments with various settings and parameters. After selecting the best performing run, we'll create a trained model and save it for operationalization in the next phase."
            ],
            "metadata": {
                "azdata_cell_guid": "6aa0ed20-d30b-40a9-86b5-db2231a18b04"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Fill all NA values with 0:\r\n",
                "df = df.fillna(0)\r\n",
                "\r\n",
                "# Drop all duplicate observations:\r\n",
                "df = df.drop_duplicates()\r\n",
                "\r\n",
                "# We don't need the 'year\" or 'month' variables\r\n",
                "df = df.drop('year', 1)\r\n",
                "df = df.drop('month', 1)\r\n",
                "\r\n",
                "# Implement One-Hot Encoding for this model (https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/) \r\n",
                "columns_to_encode = list(df.select_dtypes(include=['category','object']))\r\n",
                "dummies = pd.get_dummies(df[columns_to_encode]) #\r\n",
                "\r\n",
                "# Drop the original categorical columns:\r\n",
                "df = df.drop(columns_to_encode, axis=1) # \r\n",
                "\r\n",
                "# Re-join the dummies frame to the original data:\r\n",
                "df = df.join(dummies)\r\n",
                "\r\n",
                "# Show the new columns in the joined dataframe:\r\n",
                "print(df.columns, '\\n')\r\n",
                "\r\n",
                "# Experiment using Naive Bayes:\r\n",
                "nb_model = GaussianNB()\r\n",
                "random_seed = 42\r\n",
                "split_ratio = .3\r\n",
                "train, test = train_test_split(df, random_state = random_seed, test_size = split_ratio)\r\n",
                "\r\n",
                "target = train['churn'].values\r\n",
                "train = train.drop('churn', 1)\r\n",
                "train = train.values\r\n",
                "nb_model.fit(train, target)\r\n",
                "\r\n",
                "# Compare training versus known values\r\n",
                "expected = test['churn'].values\r\n",
                "test = test.drop('churn', 1)\r\n",
                "predicted = nb_model.predict(test)\r\n",
                "\r\n",
                "# Print out the Naive Bayes Classification Accuracy:\r\n",
                "print(\"Naive Bayes Classification Accuracy\", accuracy_score(expected, predicted))\r\n",
                "\r\n",
                "# Experiment using Decision Trees:\r\n",
                "dt_model = DecisionTreeClassifier(min_samples_split=20, random_state=99)\r\n",
                "dt_model.fit(train, target)\r\n",
                "predicted = dt_model.predict(test)\r\n",
                "\r\n",
                "# Print out the Decision Tree Accuracy:\r\n",
                "print(\"Decision Tree Classification Accuracy\", accuracy_score(expected, predicted))\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "ae055541-bb0e-4db8-9ee1-852842d11ad4",
                "tags": []
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Index(['age', 'annualincome', 'calldroprate', 'callfailurerate', 'callingnum',\n       'customerid', 'monthlybilledamount', 'numberofcomplaints',\n       'numberofmonthunpaid', 'numdayscontractequipmentplanexpiring',\n       'penaltytoswitch', 'totalminsusedinlastmonth', 'unpaidbalance',\n       'percentagecalloutsidenetwork', 'totalcallduration', 'avgcallduration',\n       'churn', 'customersuspended_No', 'customersuspended_Yes',\n       'education_Bachelor or equivalent', 'education_High School or below',\n       'education_Master or equivalent', 'education_PhD or equivalent',\n       'gender_Female', 'gender_Male', 'homeowner_No', 'homeowner_Yes',\n       'maritalstatus_Married', 'maritalstatus_Single', 'noadditionallines_\\N',\n       'occupation_Non-technology Related Job', 'occupation_Others',\n       'occupation_Technology Related Job', 'state_AK', 'state_AL', 'state_AR',\n       'state_AZ', 'state_CA', 'state_CO', 'state_CT', 'state_DE', 'state_FL',\n       'state_GA', 'state_HI', 'state_IA', 'state_ID', 'state_IL', 'state_IN',\n       'state_KS', 'state_KY', 'state_LA', 'state_MA', 'state_MD', 'state_ME',\n       'state_MI', 'state_MN', 'state_MO', 'state_MS', 'state_MT', 'state_NC',\n       'state_ND', 'state_NE', 'state_NH', 'state_NJ', 'state_NM', 'state_NV',\n       'state_NY', 'state_OH', 'state_OK', 'state_OR', 'state_PA', 'state_RI',\n       'state_SC', 'state_SD', 'state_TN', 'state_TX', 'state_UT', 'state_VA',\n       'state_VT', 'state_WA', 'state_WI', 'state_WV', 'state_WY',\n       'usesinternetservice_No', 'usesinternetservice_Yes',\n       'usesvoiceservice_No', 'usesvoiceservice_Yes'],\n      dtype='object') \n\nNaive Bayes Classification Accuracy 0.9008304836345872\n"
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Decision Tree Classification Accuracy 0.9091353199804592\n"
                }
            ],
            "execution_count": 16
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Phase 4: Deployment\r\n",
                "\r\n",
                "Once you are satisfied with the Model, you can save it out using the \"Pickle\" library for deployment to other systems."
            ],
            "metadata": {
                "azdata_cell_guid": "ed45018c-a3d5-4724-97b6-b036572449c8"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# serialize the best performing model on disk\r\n",
                "print (\"Serialize the model to a model.pkl file in the root\")\r\n",
                "ModelFile = open('./model.pkl', 'wb')\r\n",
                "pickle.dump(dt_model, ModelFile)\r\n",
                "ModelFile.close()\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "43e5058a-d5c3-4a8f-b037-b4a669d78a7f"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Serialize the model to a model.pkl file in the root\n"
                }
            ],
            "execution_count": 17
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Phase 5: Customer Acceptance\r\n",
                "\r\n",
                "The final phase involves testing the model predictions on real-world queries to ensure that it meets all requirements. In this pase we also document the project so that all parameters are well-known. Finally, a mechanism is created to re-train the model. In this code, we'll open the model we deployed, send it some new data, and predict whether a customer (which the model has never seen) will leave the product or not, allowing a salesperson to concentrate on retaining them:\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "38721dc9-9116-4178-92f8-4beafa4fd0e3"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Prepare the web service definition before deploying\r\n",
                "# Import for the pickle\r\n",
                "from sklearn.externals import joblib\r\n",
                "\r\n",
                "# load the model file\r\n",
                "global model\r\n",
                "model = joblib.load('model.pkl')\r\n",
                "\r\n",
                "# Import for handling the JSON file\r\n",
                "import json\r\n",
                "import pandas as pd\r\n",
                "\r\n",
                "# Set up a sample \"call\" from a client:\r\n",
                "input_df = \"{\\\"callfailurerate\\\": 0, \\\"education\\\": \\\"Bachelor or equivalent\\\", \\\"usesinternetservice\\\": \\\"No\\\", \\\"gender\\\": \\\"Male\\\", \\\"unpaidbalance\\\": 19, \\\"occupation\\\": \\\"Technology Related Job\\\", \\\"year\\\": 2015, \\\"numberofcomplaints\\\": 0, \\\"avgcallduration\\\": 663, \\\"usesvoiceservice\\\": \\\"No\\\", \\\"annualincome\\\": 168147, \\\"totalminsusedinlastmonth\\\": 15, \\\"homeowner\\\": \\\"Yes\\\", \\\"age\\\": 12, \\\"maritalstatus\\\": \\\"Single\\\", \\\"month\\\": 1, \\\"calldroprate\\\": 0.06, \\\"percentagecalloutsidenetwork\\\": 0.82, \\\"penaltytoswitch\\\": 371, \\\"monthlybilledamount\\\": 71, \\\"churn\\\": 0, \\\"numdayscontractequipmentplanexpiring\\\": 96, \\\"totalcallduration\\\": 5971, \\\"callingnum\\\": 4251078442, \\\"state\\\": \\\"WA\\\", \\\"customerid\\\": 1, \\\"customersuspended\\\": \\\"Yes\\\", \\\"numberofmonthunpaid\\\": 7, \\\"noadditionallines\\\": \\\"\\\\\\\\N\\\"}\"\r\n",
                "\r\n",
                "# Cleanup \r\n",
                "input_df_encoded = json.loads(input_df)\r\n",
                "input_df_encoded = pd.DataFrame([input_df_encoded], columns=input_df_encoded.keys())\r\n",
                "input_df_encoded = input_df_encoded.drop('year', 1)\r\n",
                "input_df_encoded = input_df_encoded.drop('month', 1)\r\n",
                "input_df_encoded = input_df_encoded.drop('churn', 1)\r\n",
                "\r\n",
                "# Pre-process scoring data consistent with training data\r\n",
                "columns_to_encode = ['customersuspended', 'education', 'gender', 'homeowner', 'maritalstatus', 'noadditionallines', 'occupation', 'state', 'usesinternetservice', 'usesvoiceservice']\r\n",
                "dummies = pd.get_dummies(input_df_encoded[columns_to_encode])\r\n",
                "input_df_encoded = input_df_encoded.join(dummies)\r\n",
                "input_df_encoded = input_df_encoded.drop(columns_to_encode, axis=1)\r\n",
                "\r\n",
                "columns_encoded = ['age', 'annualincome', 'calldroprate', 'callfailurerate', 'callingnum',\r\n",
                "       'customerid', 'monthlybilledamount', 'numberofcomplaints',\r\n",
                "       'numberofmonthunpaid', 'numdayscontractequipmentplanexpiring',\r\n",
                "       'penaltytoswitch', 'totalminsusedinlastmonth', 'unpaidbalance',\r\n",
                "       'percentagecalloutsidenetwork', 'totalcallduration', 'avgcallduration',\r\n",
                "       'customersuspended_No', 'customersuspended_Yes',\r\n",
                "       'education_Bachelor or equivalent', 'education_High School or below',\r\n",
                "       'education_Master or equivalent', 'education_PhD or equivalent',\r\n",
                "       'gender_Female', 'gender_Male', 'homeowner_No', 'homeowner_Yes',\r\n",
                "       'maritalstatus_Married', 'maritalstatus_Single', 'noadditionallines_\\\\N',\r\n",
                "       'occupation_Non-technology Related Job', 'occupation_Others',\r\n",
                "       'occupation_Technology Related Job', 'state_AK', 'state_AL', 'state_AR',\r\n",
                "       'state_AZ', 'state_CA', 'state_CO', 'state_CT', 'state_DE', 'state_FL',\r\n",
                "       'state_GA', 'state_HI', 'state_IA', 'state_ID', 'state_IL', 'state_IN',\r\n",
                "       'state_KS', 'state_KY', 'state_LA', 'state_MA', 'state_MD', 'state_ME',\r\n",
                "       'state_MI', 'state_MN', 'state_MO', 'state_MS', 'state_MT', 'state_NC',\r\n",
                "       'state_ND', 'state_NE', 'state_NH', 'state_NJ', 'state_NM', 'state_NV',\r\n",
                "       'state_NY', 'state_OH', 'state_OK', 'state_OR', 'state_PA', 'state_RI',\r\n",
                "       'state_SC', 'state_SD', 'state_TN', 'state_TX', 'state_UT', 'state_VA',\r\n",
                "       'state_VT', 'state_WA', 'state_WI', 'state_WV', 'state_WY',\r\n",
                "       'usesinternetservice_No', 'usesinternetservice_Yes',\r\n",
                "       'usesvoiceservice_No', 'usesvoiceservice_Yes']\r\n",
                "\r\n",
                "# Now that they are encoded, some values will be \"empty\". Fill those with 0's:\r\n",
                "for column_encoded in columns_encoded:\r\n",
                "    if not column_encoded in input_df_encoded.columns:\r\n",
                "        input_df_encoded[column_encoded] = 0\r\n",
                "\r\n",
                "# Return final prediction\r\n",
                "pred = model.predict(input_df_encoded)\r\n",
                "\r\n",
                "# (In production you would replace Print() statement here with some sort of return to JSON)\r\n",
                "print('JSON sent to the prediction Model:', '\\n')\r\n",
                "print(input_df, '\\n')\r\n",
                "print('For the JSON string sent from the client, The prediction is returned as more JSON (0 = No churn, 1 = Churn):', '\\n')\r\n",
                "print(json.dumps(str(pred[0])))"
            ],
            "metadata": {
                "azdata_cell_guid": "ed1fd784-d8b9-4c45-97f9-e834fa113203"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "JSON sent to the prediction Model: \n\n{\"callfailurerate\": 0, \"education\": \"Bachelor or equivalent\", \"usesinternetservice\": \"No\", \"gender\": \"Male\", \"unpaidbalance\": 19, \"occupation\": \"Technology Related Job\", \"year\": 2015, \"numberofcomplaints\": 0, \"avgcallduration\": 663, \"usesvoiceservice\": \"No\", \"annualincome\": 168147, \"totalminsusedinlastmonth\": 15, \"homeowner\": \"Yes\", \"age\": 12, \"maritalstatus\": \"Single\", \"month\": 1, \"calldroprate\": 0.06, \"percentagecalloutsidenetwork\": 0.82, \"penaltytoswitch\": 371, \"monthlybilledamount\": 71, \"churn\": 0, \"numdayscontractequipmentplanexpiring\": 96, \"totalcallduration\": 5971, \"callingnum\": 4251078442, \"state\": \"WA\", \"customerid\": 1, \"customersuspended\": \"Yes\", \"numberofmonthunpaid\": 7, \"noadditionallines\": \"\\\\N\"} \n\nFor the JSON string sent from the client, The prediction is returned as more JSON (0 = No churn, 1 = Churn): \n\n\"0\"\n"
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "c:\\users\\buck\\azuredatastudio-python\\0.0.1\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n  warnings.warn(msg, category=DeprecationWarning)\n"
                }
            ],
            "execution_count": 18
        }
    ]
}