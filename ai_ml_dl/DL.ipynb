{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.6",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "<img style=\"text-align:left;\" src=\"https://github.com/Microsoft/sqlworkshops/blob/master/graphics/solutions-microsoft-logo-small.png?raw=true\" alt=\"Microsoft\">\r\n",
                "<br>\r\n",
                "\r\n",
                "# Neural Networks and Deep Learning Example\r\n",
                "## Text Recognition with Neural Networks\r\n",
                "\r\n",
                "In this example you will get a brief introduction to Deep Learning in Python with Standard libraries. \r\n",
                "\r\n",
                "First, let's import the `numpy` library and define some Sigmoid functions we'll use in our example: \r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "9a988d2c-da29-493f-aebd-5341daa20a51"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import numpy as np\n",
                "\n",
                "def sigmoid(x):\n",
                "    return 1.0/(1 + np.exp(-x))\n",
                "\n",
                "def sigmoid_derivative(x):\n",
                "    return x * (1.0 - x)\n",
                "\n",
                "print(\"Libraries and initial functions defined.\") "
            ],
            "metadata": {
                "azdata_cell_guid": "a5252a2e-fa55-4fc0-8e7a-bb50295e1f0f"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Libraries and initial functions defined.\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 2
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Create Neural Network Class\r\n",
                "\r\n",
                "The next thing we need is a Neural Network (*Sometimes called an Artificial Neural Network or ANN*) to perform the shape recognition. ([You can learn more about Neural Networks here](http://pages.cs.wisc.edu/~bolo/shipyard/neural/local.html)). \r\n",
                "\r\n",
                "Bascially, a Neural Network takes inputs, moves those values through *layers*, and then sends along the output as a prediction. Another important concept is the idea that the `Nodes` (functions within the layers) can have `weights` which are values that affect the calculations done at the Node. ([More on Weights and Bias here](https://medium.com/fintechexplained/neural-networks-bias-and-weights-10b53e6285da))\r\n",
                "\r\n",
                "<img src=\"https://upload.wikimedia.org/wikipedia/en/5/54/Feed_forward_neural_net.gif\" height=\"152\" alt=\"Layers\">\r\n",
                "\r\n",
                "*Image credit: Wikipedia*\r\n",
                "\r\n",
                "Let's set up a Class in Python for our `ANN` code:\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "d2b2ad21-48c1-46d1-bcb2-af795898d4ca"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "class NeuralNetwork:\n",
                "    def __init__(self, x, y):\n",
                "        self.input      = x\n",
                "        self.weights1   = np.random.rand(self.input.shape[1],4) \n",
                "        self.weights2   = np.random.rand(4,1)                 \n",
                "        self.y          = y\n",
                "        self.output     = np.zeros(y.shape)\n",
                "\n",
                "print(\"Initial Neural Network Class created.\")"
            ],
            "metadata": {
                "azdata_cell_guid": "66580be0-c709-41ff-a0f3-6e8b89b4cbd3"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Initial Neural Network Class created.\n"
                }
            ],
            "execution_count": 2
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Adding feedforward to a 2-Layer ANN \n",
                "\n",
                "Now we'll add some functionality to our `ANN` Class. One of the earliest types of ANN's was the [feedforward neural network](https://en.wikipedia.org/wiki/Feedforward_neural_network). This network moves the data from the input (*input nodes*) through various layers of algorithms (*hidden nodes*) and then on to the output (*output nodes*) in only one direction- it does not cycle or loop back to any nodes.\n",
                "\n",
                "The simplest version of these moves directly from the input to one layer of Nodes and then on to the output. We'll need a more complex ANN, [called a 2-Layer (or multi-layer) ANN](https://youtu.be/s8pDf2Pt9sc).\n",
                "\n",
                "Using this kind of ANN allows us to evaluate the shapes in the inputs we'll send, along with the Weights we want to set up: "
            ],
            "metadata": {
                "azdata_cell_guid": "77fb0012-fdab-4f2c-9003-3d8b5321e394"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "class NeuralNetwork:\n",
                "    def __init__(self, x, y):\n",
                "        self.input      = x\n",
                "        self.weights1   = np.random.rand(self.input.shape[1],4) \n",
                "        self.weights2   = np.random.rand(9,1)                 \n",
                "        self.y          = y\n",
                "        self.output     = np.zeros(y.shape)\n",
                "    def feedforward(self):\n",
                "        # assumes bias is zero\n",
                "        # 2-layer network\n",
                "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
                "        self.output = sigmoid(np.dot(self.layer1, self.weights2))\n",
                "\n",
                "print(\"feedforward and 0-bias added to the ANN.\")"
            ],
            "metadata": {
                "azdata_cell_guid": "03556b1e-d091-4e53-a2bf-0f6f51c6b7f0"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "feedforward and 0-bias added to the ANN.\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 3
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Setting up the Loss Functions, Back-Propigation, Gradient Decent, and the Chain Rules\n",
                "\n",
                "A `Loss Function` is another important concpet in `ANN's`. A [Loss Function](https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/) measures the difference between the predicted value of the function and the `label` or true value. The lower the value, the better the model. \n",
                "\n",
                "As the data moves through the model, the model itself can use the previous result from a layer to tune itself by fine-tuning the weights the layer recieves. This is called *Back-Propagation*.  ([Learn more about Back-Propagation here](https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/))\n",
                "\n",
                "To get to the prediction in the fastest way possible, we'll use a `Gradient Descent` to help gthe Loss Function. It's based on a mathematical principal that minimizes the distance to get to a point. [You can read more about Gradient Descent here.](https://peterroelants.github.io/posts/neural-network-implementation-part01/)\n",
                "\n",
                "Putting this all together, we use the `Chain Rule`, which is a format of programming that uses derivatives to \"chain\" the computation. It's a bit complex, so [here's a fun video that helps with understanding it](https://www.youtube.com/watch?v=sDv4f4s2SB8). \n",
                "\n",
                "OK - putting that into code, lets create our final Neural Network class:\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "5fa15737-1ef8-4097-bd97-3956f7bd5f9d"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "class NeuralNetwork:\n",
                "    def __init__(self, x, y):\n",
                "        self.input      = x\n",
                "        self.weights1   = np.random.rand(self.input.shape[1],9) \n",
                "        self.weights2   = np.random.rand(9,1)                 \n",
                "        self.y          = y\n",
                "        self.output     = np.zeros(y.shape)\n",
                "    \n",
                "    def feedforward(self):\n",
                "        # assumes bias is zero\n",
                "        # 2-layer network\n",
                "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
                "        self.output = sigmoid(np.dot(self.layer1, self.weights2))\n",
                "    \n",
                "    def backprop(self):\n",
                "        # application of the chain rule to find derivative of the loss function with respect to weights2 and weights1\n",
                "        d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivative(self.output)))\n",
                "        d_weights1 = np.dot(self.input.T,  (np.dot(2*(self.y - self.output) * sigmoid_derivative(self.output), self.weights2.T) * sigmoid_derivative(self.layer1)))\n",
                "        # update the weights with the derivative (slope) of the loss function\n",
                "        self.weights1 += d_weights1\n",
                "        self.weights2 += d_weights2\n",
                "\n",
                "print(\"Chain Rule created for ANN with backprop added.\")"
            ],
            "metadata": {
                "azdata_cell_guid": "c6d15516-38ce-4d3d-8614-d574cb921bd4"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Chain Rule created for ANN with backprop added.\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 4
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Using the ANN to Predict a Graphical Letter\n",
                "\n",
                "Our Neural Network is ready to work - let's take a typical example for Deep Learning: Image Processing. One of the earliest experiments was to have a set of hand-written letters and numbers (like those on an envelope) and attempt to have the computer figure out the text even though it is written in several styles. The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that we can use to train our ANN, and then we can use that to figure out whether a letter is an \"L\" or not. \n",
                "\n",
                "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\" height=\"152\" alt=\"Layers\">\n",
                "\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "552a31fd-0137-4472-8f68-26e33ba1b95e"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "We've taken the first step and \"cut out\" the image of the letter \"L\" below - and some that are not L's. Visually, you can instantly tell whether the image is an L or not, but your brain is using several systems (ocular, brain, training and others) to know that. A computer has to do it with math, and specifically our ANN we built. To do that, we'll have to take these images, and convert them to numbers.\n",
                "\n",
                "Here are the raw images, cut out and boxed:\n",
                "\n",
                "<img src=\"https://github.com/microsoft/sqlworkshops/blob/master/graphics/ML-AI-DL-Letters.png?raw=TRUE\" height=\"152\" alt=\"Layers\">\n",
                "\n",
                "To make numbers out of them, let's first put them in a grid:\n",
                "\n",
                "<img src=\"https://github.com/microsoft/sqlworkshops/blob/master/graphics/ML-AI-DL-Letters-Blocked.png?raw=TRUE\" height=\"152\" alt=\"Layers\">\n",
                "\n",
                "So now we have five two-dimensional arrays of numbers we could put in the blocks. But we would like a single array, to have the ANN guess at all of them at once.  Here's what we can do: \n",
                "\n",
                "We can \"unroll\" each block top to bottom, and put them on a single line. Then we can take that line, and put numbers showing whether a block is filled or not. Then we can stack all of the lines of unrolled boxes into a single array. Here's what those three steps look like:\n",
                "\n",
                "<img src=\"https://github.com/microsoft/sqlworkshops/blob/master/graphics/ML-AI-DL-Letters-Array.png?raw=TRUE\" height=\"350\" alt=\"Layers\">\n",
                "\n",
                "Now we have a single array (in some programs, a *tensor*) that we can send to the ANN as an input. "
            ],
            "metadata": {
                "azdata_cell_guid": "ab55d41c-d017-496a-a8e5-243c050015bb"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Setting up the data to send to the ANN\r\n",
                "\r\n",
                "So now we have our letters converted to letters for training, and we can add one additional piece of information: We know whether it's really an **L** or not. This is a \"Label\" that we want to find. Now we can train the network for what we want to predict. "
            ],
            "metadata": {
                "azdata_cell_guid": "018af13d-90d3-4b82-a72f-6f6269240409"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "dataset = [[0,1,0,0,1,1,0,0,0,1],\n",
                "          [1,0,0,1,1,0,0,0,0, 1],\n",
                "          [0,0,1,0,1,1,0,0,0, 0],\n",
                "          [0,0,0,0,1,0,1,1,0, 0],\n",
                "          [0,0,0,1,0,0,1,1,0, 1]\n",
                "          ]\n",
                "dataset = np.array(dataset)\n",
                "print('Here is the full set of data, along with the \"Label\" at the end: \\n', dataset)\n",
                "\n",
                "X = dataset[:,:9]\n",
                "print('Here are the \"Features\" in the data set: \\n', X)\n",
                "\n",
                "Y = np.array([[d] for d in dataset[:,-1]])\n",
                "print('And here are the \"Labels\" in the data set: \\n', Y)"
            ],
            "metadata": {
                "azdata_cell_guid": "8ee196e8-f764-4f82-a6c6-569ced05ecb8"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Here is the full set of data, along with the \"Label\" at the end: \n [[0 1 0 0 1 1 0 0 0 1]\n [1 0 0 1 1 0 0 0 0 1]\n [0 0 1 0 1 1 0 0 0 0]\n [0 0 0 0 1 0 1 1 0 0]\n [0 0 0 1 0 0 1 1 0 1]]\nHere are the \"Features\" in the data set: \n [[0 1 0 0 1 1 0 0 0]\n [1 0 0 1 1 0 0 0 0]\n [0 0 1 0 1 1 0 0 0]\n [0 0 0 0 1 0 1 1 0]\n [0 0 0 1 0 0 1 1 0]]\nAnd here are the \"Labels\" in the data set: \n [[1]\n [1]\n [0]\n [0]\n [1]]\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 5
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Instantiate the ANN using the Data\r\n",
                "\r\n",
                "With the X's and Y's defined as input, we can now use the Neural Network. Let's send in the data:"
            ],
            "metadata": {
                "azdata_cell_guid": "d432287e-57bd-4dbc-9776-e511e9ed734c"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "nn = NeuralNetwork(X,Y)\n",
                "print(\"Created\", nn)"
            ],
            "metadata": {
                "azdata_cell_guid": "e4b91abc-d42a-4f5a-8be2-3abb6379118a"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Created <__main__.NeuralNetwork object at 0x000002952D1B02E8>\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 6
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Add in feedfoward and back-propagation\r\n",
                "\r\n",
                "As our final step, we will iterate the feedforward steps and the back-propagation we saw earlier. We'll then print out the predictions:"
            ],
            "metadata": {
                "azdata_cell_guid": "b6b1db07-44bf-422a-a6ff-63767febbe62"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "for i in range(1500):\n",
                "    nn.feedforward()\n",
                "    nn.backprop()\n",
                "\n",
                "for i in range(0,5):\n",
                "    print('The 2-layer Neural Network is',  round(nn.output[i][0]*100,2), '% sure that image', i, 'is an \"L\"')"
            ],
            "metadata": {
                "azdata_cell_guid": "737c2407-db8a-43da-8d32-cdf6d250d550"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "The 2-layer Neural Network is 99.75 % sure that image 0 is an \"L\"\nThe 2-layer Neural Network is 99.95 % sure that image 1 is an \"L\"\nThe 2-layer Neural Network is 0.23 % sure that image 2 is an \"L\"\nThe 2-layer Neural Network is 0.31 % sure that image 3 is an \"L\"\nThe 2-layer Neural Network is 99.75 % sure that image 4 is an \"L\"\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 14
        },
        {
            "cell_type": "markdown",
            "source": [
                "Reference: [How to build your own Neural Network from scratch in Python](https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6)"
            ],
            "metadata": {
                "azdata_cell_guid": "3f34bee3-59f2-4540-b1ff-b53c040f7b44"
            }
        }
    ]
}